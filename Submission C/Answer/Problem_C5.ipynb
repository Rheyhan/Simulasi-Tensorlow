{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# PROBLEM C5\n",
    "#\n",
    "# Build and train a neural network to predict time indexed variables of\n",
    "# the multivariate house hold electric power consumption time series dataset.\n",
    "# Using a window of past 24 observations of the 7 variables, the model \n",
    "# should be trained to predict the next 24 observations of the 7 variables.\n",
    "# Use MAE as the metrics of your neural network model.\n",
    "# We provided code for normalizing the data. Please do not change the code.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is downloaded from https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption\n",
    "#\n",
    "# Desired MAE < 0.1 on the normalized dataset.\n",
    "# ============================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 14:53:07.571425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-17 14:53:09.456612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_data():\n",
    "    url = 'https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/household_power.zip'\n",
    "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
    "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "\n",
    "# This function normalizes the dataset using min max scaling.\n",
    "# DO NOT CHANGE THIS CODE\n",
    "def normalize_series(data, min, max):\n",
    "    data = data - min\n",
    "    data = data / max\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35254/3320084278.py:4: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv('household_power_consumption.csv', sep=',',\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "download_and_extract_data()\n",
    "# Reads the dataset from the csv.\n",
    "df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
    "                 infer_datetime_format=True, index_col='datetime', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURES = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "data = df.values\n",
    "split_time = int(len(data) * 0.5)\n",
    "data = normalize_series(data, data.min(axis=0), data.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into training and validation sets.\n",
    "x_train = data[:split_time]\n",
    "x_valid = data[split_time:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS\n",
    "BATCH_SIZE = 32  \n",
    "N_PAST = 24 # Number of past time steps based on which future observations should be predicted\n",
    "N_FUTURE = 24  # Number of future time steps which are to be predicted.\n",
    "SHIFT = 1  # By how many positions the window slides to create a new window of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, batch_size=BATCH_SIZE, n_past=24, n_future=24, shift=1):\n",
    "    window_size=n_past+n_future\n",
    "    \n",
    "    # Turn into tensor dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # window of the data\n",
    "    ds = ds.window(window_size, shift=shift, drop_remainder=True)\n",
    "    \n",
    "    # flatten windows into individual elements\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    \n",
    "    # Split windows into input (past) and label (future) sets\n",
    "    ds = ds.map(lambda x: (x[:n_past], x[n_past:]))\n",
    "    \n",
    "    # batch ds    \n",
    "    ds = ds.batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 14:53:29.751326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:29.984770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:29.984825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:29.987885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:29.987979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:29.988012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:30.560166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:30.560293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:30.560303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-17 14:53:30.560342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-17 14:53:30.560424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Code to create windowed train and validation datasets.\n",
    "# Complete the code in windowed_dataset.\n",
    "train_set = windowed_dataset(x_train)\n",
    "valid_set = windowed_dataset(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class thecustomcallbacks(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(thecustomcallbacks, self).__init__()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loggedtrain = logs[\"MAE\"]\n",
    "        if loggedtrain < 0.1:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(input_shape=(N_PAST,N_FEATURES), units=64, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(N_FEATURES),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\"adam\", loss=tf.keras.losses.MeanSquaredError(),metrics=\"MAE\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 14:54:15.902600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-02-17 14:54:16.042955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-02-17 14:54:16.086060: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5f7c0366c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-17 14:54:16.086114: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-02-17 14:54:16.142167: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-17 14:54:16.465608: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-17 14:54:16.596929: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1349/Unknown - 30s 18ms/step - loss: 0.0239 - MAE: 0.0863"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 14:54:42.133295: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11443001950356000277\n",
      "2024-02-17 14:54:42.133372: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1636632731279744914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1349/1349 [==============================] - 43s 28ms/step - loss: 0.0239 - MAE: 0.0863 - val_loss: 0.0205 - val_MAE: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 14:54:55.688261: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11443001950356000277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f6050982c70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=get_model()\n",
    "model.fit(train_set, validation_data=valid_set, callbacks=thecustomcallbacks(), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35254/3279125998.py:66: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv('household_power_consumption.csv', sep=',',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "   1349/Unknown - 26s 17ms/step - loss: 0.0239 - MAE: 0.0860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 14:58:01.628080: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 2938959074362778788\n",
      "2024-02-17 14:58:01.628150: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 11127073154597915652\n",
      "2024-02-17 14:58:01.628181: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11443001950356000277\n",
      "2024-02-17 14:58:01.628220: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 4625381010506968607\n",
      "2024-02-17 14:58:01.628251: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1636632731279744914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1349/1349 [==============================] - 38s 26ms/step - loss: 0.0239 - MAE: 0.0860 - val_loss: 0.0205 - val_MAE: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/conda3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================================\n",
    "# PROBLEM C5\n",
    "#\n",
    "# Build and train a neural network to predict time indexed variables of\n",
    "# the multivariate house hold electric power consumption time series dataset.\n",
    "# Using a window of past 24 observations of the 7 variables, the model \n",
    "# should be trained to predict the next 24 observations of the 7 variables.\n",
    "# Use MAE as the metrics of your neural network model.\n",
    "# We provided code for normalizing the data. Please do not change the code.\n",
    "# Do not use lambda layers in your model.\n",
    "#\n",
    "# The dataset used in this problem is downloaded from https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption\n",
    "#\n",
    "# Desired MAE < 0.1 on the normalized dataset.\n",
    "# ============================================================================================\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# This function downloads and extracts the dataset to the directory that contains this file.\n",
    "# DO NOT CHANGE THIS CODE\n",
    "# (unless you need to change the URL)\n",
    "def download_and_extract_data():\n",
    "    url = 'https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/household_power.zip'\n",
    "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
    "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "\n",
    "# This function normalizes the dataset using min max scaling.\n",
    "# DO NOT CHANGE THIS CODE\n",
    "def normalize_series(data, min, max):\n",
    "    data = data - min\n",
    "    data = data / max\n",
    "    return data\n",
    "\n",
    "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
    "def windowed_dataset(series, batch_size=BATCH_SIZE, n_past=24, n_future=24, shift=1):\n",
    "    window_size=n_past+n_future\n",
    "    \n",
    "    # Turn into tensor dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # window of the data\n",
    "    ds = ds.window(window_size, shift=shift, drop_remainder=True)\n",
    "    \n",
    "    # flatten windows into individual elements\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    \n",
    "    # Split windows into input (past) and label (future) sets\n",
    "    ds = ds.map(lambda x: (x[:n_past], x[n_past:]))\n",
    "    \n",
    "    # batch ds    \n",
    "    ds = ds.batch(batch_size=batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
    "def solution_C5():\n",
    "    # Downloads and extracts the dataset to the directory that contains this file.\n",
    "    download_and_extract_data()\n",
    "    # Reads the dataset from the csv.\n",
    "    df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
    "                     infer_datetime_format=True, index_col='datetime', header=0)\n",
    "\n",
    "    # Number of features in the dataset. We use all features as predictors to\n",
    "    # predict all features at future time steps.\n",
    "    N_FEATURES = df.shape[1]\n",
    "\n",
    "    # Normalizes the data\n",
    "    # DO NOT CHANGE THIS\n",
    "    data = df.values\n",
    "    split_time = int(len(data) * 0.5)\n",
    "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
    "\n",
    "    # Splits the data into training and validation sets.\n",
    "    x_train = data[:split_time]\n",
    "    x_valid = data[split_time:]\n",
    "\n",
    "    # DO NOT CHANGE THIS\n",
    "    BATCH_SIZE = 32  \n",
    "    N_PAST = 24 # Number of past time steps based on which future observations should be predicted\n",
    "    N_FUTURE = 24  # Number of future time steps which are to be predicted.\n",
    "    SHIFT = 1  # By how many positions the window slides to create a new window of observations.\n",
    "\n",
    "    # Code to create windowed train and validation datasets.\n",
    "    # Complete the code in windowed_dataset.\n",
    "    train_set = windowed_dataset(x_train)\n",
    "    valid_set = windowed_dataset(x_valid)\n",
    "\n",
    "    class thecustomcallbacks(tf.keras.callbacks.Callback):\n",
    "        def __init__(self):\n",
    "            super(thecustomcallbacks, self).__init__()\n",
    "        \n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            loggedtrain = logs[\"MAE\"]\n",
    "            if loggedtrain < 0.1:\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def get_model():\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.LSTM(input_shape=(N_PAST,N_FEATURES), units=64, return_sequences=True),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.LSTM(units=128, return_sequences=True),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(N_FEATURES),\n",
    "        ])\n",
    "        \n",
    "        model.compile(\"adam\", loss=tf.keras.losses.MeanSquaredError(),metrics=\"MAE\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    model=get_model()\n",
    "    model.fit(train_set, validation_data=valid_set, callbacks=thecustomcallbacks(), epochs=100)\n",
    "\n",
    "    return model\n",
    "\n",
    "# The code below is to save your model as a .h5 file.\n",
    "# It will be saved automatically in your Submission folder.\n",
    "if __name__ == '__main__':\n",
    "    # DO NOT CHANGE THIS CODE\n",
    "    model = solution_C5()\n",
    "    model.save(\"Model/model_C5.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
